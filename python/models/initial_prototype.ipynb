{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678aece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking V14 as final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c37e674",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517a5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter\n",
    "from tkinter import *\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import messagebox\n",
    "from tkinter import ttk\n",
    "import praw #Python Reddit API Wrapper\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk #Natural Language Toolkit\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer  # Added for sentiment analysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Download NLTK stopwords and Vader Lexicon if not already downloaded\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e3120b",
   "metadata": {},
   "source": [
    "# **Sentiment Analyzer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7504efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # Added for sentiment analysis\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to analyze the sentiment of a text using VADER\n",
    "def perform_sentiment_analysis(data, threshold=0.3):\n",
    "    sentiments = []\n",
    "\n",
    "    for post in data.itertuples():\n",
    "        text = post.title + \" \" + post.body\n",
    "\n",
    "        # Perform sentiment analysis using VADER\n",
    "        sentiment_scores = analyzer.polarity_scores(text)\n",
    "        compound_score = sentiment_scores['compound']\n",
    "        \n",
    "        if compound_score >= threshold:\n",
    "            sentiment_label = 'Positive'\n",
    "        elif compound_score <= -threshold:\n",
    "            sentiment_label = 'Negative'\n",
    "        else:\n",
    "            sentiment_label = 'Neutral'\n",
    "\n",
    "        sentiments.append({\n",
    "            \"text\": text,\n",
    "            \"sentiment\": sentiment_label,\n",
    "            \"score\": compound_score\n",
    "        })\n",
    "\n",
    "    return sentiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e24a57",
   "metadata": {},
   "source": [
    "# **Post Classification Fn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e973e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_post_media(post):\n",
    "    if post.url.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
    "        return 'Image'\n",
    "    elif 'v.redd.it' in post.url:\n",
    "        return 'Video'\n",
    "    else:\n",
    "        return 'Text'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff16cc0",
   "metadata": {},
   "source": [
    "# Reddit Search Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4c8fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import string\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "def reddit_search():\n",
    "    tb.delete('1.0', END)\n",
    "    flag = 0\n",
    "    test = ''\n",
    "    reddit = praw.Reddit(client_id='DP78tG9HeZiMQg', client_secret='xF80XIHboP51Lq63viNLTzxJrmE', user_agent='RedditWebScraping')\n",
    "    \n",
    "    # Getting title from input\n",
    "    Sub = E1.get()\n",
    "\n",
    "    tb.insert(INSERT, '\\n------------------------------Subreddit-------------------------------------\\n')\n",
    "    # Get 10000 hot posts from the given subreddit\n",
    "    hot_posts = reddit.subreddit(Sub).hot(limit=10000)\n",
    "    try:\n",
    "        for post in hot_posts:\n",
    "            tb.insert(INSERT, post.title)\n",
    "            tb.insert(INSERT, \"\\n\")\n",
    "    except TclError:\n",
    "        pass\n",
    "\n",
    "    # Store the data from the posts\n",
    "    posts = []\n",
    "    this_subreddit = reddit.subreddit(Sub)\n",
    "    for post in this_subreddit.hot(limit=10000):\n",
    "        posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "    posts = pd.DataFrame(posts, columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "    \n",
    "    def get_date(created):\n",
    "        return dt.datetime.fromtimestamp(created)\n",
    "\n",
    "    try:\n",
    "        tb.insert(INSERT, posts)\n",
    "        _timestamp = posts[\"created\"].apply(get_date)\n",
    "        posts = posts.assign(timestamp=_timestamp)\n",
    "        tb.insert(INSERT, posts[['title', 'score', 'timestamp']])\n",
    "        tb.insert(INSERT, \"\\n\")\n",
    "        posts['interaction'] = posts['score'].divide(posts['num_comments'], fill_value=1)\n",
    "        tb.insert(INSERT, posts[['title', 'score', 'interaction']])\n",
    "        tb.insert(INSERT, \"\\n\")\n",
    "    except TclError:\n",
    "        pass\n",
    "    \n",
    "    # Text numerical analysis(new)\n",
    "    # Define the number of posts to process\n",
    "    limit_posts = 10000\n",
    "\n",
    "    # Initialize the Counter for word frequencies\n",
    "    word_counts = Counter()\n",
    "\n",
    "    # Load the English language model from spaCy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Define a set of punctuation characters to be removed\n",
    "    punctuations = set(string.punctuation)\n",
    "    \n",
    "    # Process the posts and update word counts\n",
    "    for post in this_subreddit.hot(limit=limit_posts):\n",
    "        title_words = nlp(post.title)\n",
    "        title_words = [word.text.lower() for word in title_words if not word.is_stop and word.text not in punctuations and \"‘\" not in word.text and \"’\" not in word.text and not word.text.isnumeric()]\n",
    "        word_counts.update(title_words)\n",
    "        print(title_words)\n",
    "\n",
    "############\n",
    "    # Get the Media Types\n",
    "    # Collect data on the type of post media\n",
    "    media_types = []    \n",
    "        \n",
    "    for post in this_subreddit.hot(limit=limit_posts):\n",
    "        media_type = classify_post_media(post)\n",
    "        media_types.append(media_type)\n",
    "    \n",
    "    # Count the number of each media type\n",
    "    media_counts = Counter(media_types)\n",
    "\n",
    "    \n",
    "############\n",
    "    # Sort and display the most important topics\n",
    "    most_common_words = word_counts.most_common(20)\n",
    "\n",
    "    finalstr = \"\\n---------------Most Important Topics----------------------\\n\"\n",
    "    for word, count in most_common_words:\n",
    "        finalstr += f\"{word}: {count}\\n\"\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "    # Insert the result into your text box \n",
    "    tb.insert(INSERT, finalstr)\n",
    "    \n",
    "    tb.insert(INSERT,'\\n--------------------------------Graphs----------------------------------->\\n')\n",
    "\n",
    "############\n",
    "    # Line Plot\n",
    "    figure1 = plt.Figure(figsize=(4,3), dpi=100)\n",
    "    ax1 = figure1.add_subplot(111)\n",
    "    line1 = FigureCanvasTkAgg(figure1, top)\n",
    "    line1.get_tk_widget().grid(row=1,column=3,columnspan=1, padx=5)\n",
    "    posts.plot(kind=\"line\",x='title',y='num_comments',color='red',ax=ax1)\n",
    "    posts.plot(kind=\"line\",x='title',y='interaction',color='blue',ax=ax1)\n",
    "    ax1.axes.get_xaxis().set_visible(False)\n",
    "    ax1.set_title('Timewise Presence Of Subreddit \\''+Sub+'\\'', fontsize=10)\n",
    "    \n",
    "    \n",
    "############\n",
    "    # Heat Map\n",
    "    # DataFrame with the selected variables\n",
    "    hm_selected_variables = posts[['timestamp', 'interaction', 'score', 'num_comments']]\n",
    "    \n",
    "    figure2 = plt.Figure(figsize=(4,3), dpi=100)\n",
    "    ax2 = figure2.add_subplot(111)\n",
    "    hm = FigureCanvasTkAgg(figure2, top) \n",
    "    hm.get_tk_widget().grid(row=1,column=4,columnspan=1, padx=5)\n",
    "    \n",
    "    # Create a heatmap of the correlation between selected variables\n",
    "    correlation_matrix = hm_selected_variables.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", ax=ax2, annot_kws={'fontsize': 6})\n",
    "    ax2.set_title('Correlation Heatmap Of Subreddit \\''+Sub+'\\'', fontsize=10)\n",
    "    \n",
    "    \n",
    "############\n",
    "    # Media Types  \n",
    "    # Display the post media distribution plot in the tkinter window\n",
    "    figure3 = plt.Figure(figsize=(4, 3), dpi=100)\n",
    "    ax3 = figure3.add_subplot(111)\n",
    "    ax3_bar = FigureCanvasTkAgg(figure3, top)\n",
    "    ax3_bar.get_tk_widget().grid(row=2, column=3, columnspan=1, padx=5)\n",
    "    ax3.bar(media_counts.keys(), media_counts.values())\n",
    "    ax3.set_xlabel('Media Type')\n",
    "    ax3.set_ylabel('No. of Posts')\n",
    "    ax3.set_title('Distribution of Media Types')\n",
    "\n",
    "\n",
    "############    \n",
    "    # Grouped bar chart\n",
    "    # DataFrame for the selected variables\n",
    "    gd_selected_variables = posts[['title', 'score', 'num_comments']]\n",
    "    \n",
    "    # Group the data by 'title' and calculate the sum of 'score' and 'num_comments' for each title\n",
    "    grouped_data = gd_selected_variables.groupby('title').sum()\n",
    "    \n",
    "     # Get the number of titles for the x-axis labels\n",
    "    num_titles = len(grouped_data)\n",
    "    \n",
    "    # Set the width of each bar and the positions for each group\n",
    "    bar_width = 0.5\n",
    "    index = range(num_titles)\n",
    "\n",
    "    # Plot a grouped bar chart\n",
    "    figure4 = plt.Figure(figsize=(4, 3), dpi=100)\n",
    "    ax4 = figure4.add_subplot(111)\n",
    "    ax4.bar(index, grouped_data['score'], bar_width, label='Score')\n",
    "    ax4.bar([i + bar_width for i in index], grouped_data['num_comments'], bar_width, label='Num Comments')\n",
    "    ax4.set_xlabel('Title')\n",
    "    ax4.set_ylabel('Value')\n",
    "    ax4.set_title('Comparison of Title,Score & No. of Comments')\n",
    "#     ax4.set_xticklabels([''] * num_titles)  # Set empty labels for the titles\n",
    "    ax4.legend()\n",
    "    chart = FigureCanvasTkAgg(figure4, top)\n",
    "    chart.get_tk_widget().grid(row=2, column=4, columnspan=1, padx=5)\n",
    "    \n",
    "\n",
    "############    \n",
    "    tb.insert(INSERT, '\\n------------------ Sentiment Analysis ------------------↓\\n\\n')\n",
    "    \n",
    "    # Perform sentiment analysis\n",
    "    sentiments = perform_sentiment_analysis(posts)\n",
    "    \n",
    "    # Pie chart for sentiment analysis results\n",
    "    sentiment_labels = [sentiment['sentiment'] for sentiment in sentiments]\n",
    "    sentiment_counts = Counter(sentiment_labels)\n",
    "    \n",
    "     # Perform sentiment analysis\n",
    "    sentiments = perform_sentiment_analysis(posts)\n",
    "    \n",
    "    # Extract positive, negative, and neutral words from the sentiment analysis results\n",
    "    positive_words = []\n",
    "    negative_words = []\n",
    "    neutral_words = []\n",
    "    \n",
    "    for sentiment in sentiments:\n",
    "        if sentiment['sentiment'] == 'Positive':\n",
    "            words = nlp(sentiment['text'])\n",
    "            for word in words:\n",
    "                if not word.is_stop and word.text not in punctuations and not word.text.isnumeric():\n",
    "                    positive_words.append(word.text)\n",
    "        elif sentiment['sentiment'] == 'Negative':\n",
    "            words = nlp(sentiment['text'])\n",
    "            for word in words:\n",
    "                if not word.is_stop and word.text not in punctuations and not word.text.isnumeric():\n",
    "                    negative_words.append(word.text)\n",
    "        else:\n",
    "            words = nlp(sentiment['text'])\n",
    "            for word in words:\n",
    "                if not word.is_stop and word.text not in punctuations and not word.text.isnumeric():\n",
    "                    neutral_words.append(word.text)\n",
    "    \n",
    "    \n",
    "    # Display the 20 most common positive, negative, and neutral words\n",
    "    tb.insert(INSERT, '------ Positive Words ------\\n')\n",
    "    tb.insert(INSERT, ', '.join(positive_words[:20]) + '\\n\\n')\n",
    "    \n",
    "    tb.insert(INSERT, '------ Negative Words ------\\n')\n",
    "    tb.insert(INSERT, ', '.join(negative_words[:20]) + '\\n\\n')\n",
    "    \n",
    "    tb.insert(INSERT, '------ Neutral Words ------\\n')\n",
    "    tb.insert(INSERT, ', '.join(neutral_words[:20]) + '\\n\\n')\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pie(sentiment_counts.values(), labels=sentiment_counts.keys(), autopct='%1.1f%%', startangle=90)\n",
    "    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    \n",
    "    # Create a FigureCanvasTkAgg to display the pie chart in the GUI\n",
    "    canvas = FigureCanvasTkAgg(fig, master=top)\n",
    "    canvas.get_tk_widget().grid(row=2, column=0, columnspan=3)\n",
    "    \n",
    "\n",
    "############\n",
    "    # WordCloud\n",
    "    dataset = str(posts.title.values)\n",
    "    wordCloudDf = pd.DataFrame(posts,columns=['title'])\n",
    "\n",
    "    comment_words = ''\n",
    "    stopstop = set(STOPWORDS) \n",
    "\n",
    "    #iterate through the csv file \n",
    "    for val in wordCloudDf.title: \n",
    "\n",
    "     # typecaste each val to string \n",
    "        val = str(val) \n",
    "\n",
    "     # split the value \n",
    "        tokens = val.split() \n",
    "\n",
    "     # Converts each token into lowercase \n",
    "        for i in range(len(tokens)): \n",
    "            tokens[i] = tokens[i].lower() \n",
    "\n",
    "        comment_words += \" \".join(tokens)+\" \"\n",
    "    \n",
    "    # Generate the WordCloud and save it to a file\n",
    "    wordcloud = WordCloud(width = 800, height = 800,background_color ='white',stopwords = stopstop, min_font_size = 10).generate(comment_words) \n",
    "    wordcloud.to_file(\"wordCloud.png\") \n",
    "    \n",
    "    # Resize the WordCloud image using Pillow\n",
    "    img = PILImage.open(\"wordCloud.png\")\n",
    "    img = img.resize((350, 300), PILImage.BICUBIC)\n",
    "    img.save(\"wordCloud_resized.png\")\n",
    "\n",
    "    # Display the resized WordCloud in the Jupyter Notebook\n",
    "    display(Image(filename=\"wordCloud_resized.png\"))    \n",
    "\n",
    "    \n",
    "top = tkinter.Tk()\n",
    "top.tk.call('encoding', 'system', 'utf-8')\n",
    "top.wm_title(\"Reddit Analysis\")\n",
    "\n",
    "# Create and place the elements in the grid\n",
    "L1 = Label(text=\"Subreddit Name\")\n",
    "L1.grid(row=0, column=0, sticky=\"nsew\")\n",
    "\n",
    "E1 = Entry(bd=5)\n",
    "E1.grid(row=0, column=1)\n",
    "\n",
    "B = tkinter.Button(text=\"Search\", command=reddit_search)\n",
    "B.grid(row=0, column=2)\n",
    "\n",
    "tb = Text(top)\n",
    "tb.grid(row=1, column=0, columnspan=3)\n",
    "\n",
    "top.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe106e",
   "metadata": {},
   "source": [
    "# Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e150b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import Image, display\n",
    "from tkinter import Label, Entry, Text, Button\n",
    "\n",
    "def reddit_graph():\n",
    "    tb.delete('1.0', tkinter.END)\n",
    "    flag = 0\n",
    "    test = ''\n",
    "    reddit = praw.Reddit(client_id='DP78tG9HeZiMQg', client_secret='xF80XIHboP51Lq63viNLTzxJrmE', user_agent='RedditWebScraping')\n",
    "\n",
    "    # Getting title from input\n",
    "    Sub = E1.get()\n",
    "\n",
    "    tb.insert(INSERT, '\\n------------------------------Subreddit-------------------------------------\\n')\n",
    "    # Get 50 hot posts from the given subreddit\n",
    "    hot_posts = reddit.subreddit(Sub).hot(limit=50)\n",
    "    \n",
    "    posts = []\n",
    "    for post in hot_posts:\n",
    "        tb.insert(INSERT, post.title)\n",
    "        tb.insert(INSERT, \"\\n\")\n",
    "\n",
    "        posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created, post.author])\n",
    "\n",
    "    posts = pd.DataFrame(posts, columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created', 'author'])\n",
    "\n",
    "    def get_date(created):\n",
    "        return pd.to_datetime(created, unit='s')\n",
    "\n",
    "    try:\n",
    "        posts['interaction'] = posts['score'] / posts['num_comments'].fillna(1)\n",
    "        tb.insert(INSERT, posts[['title', 'score', 'interaction']])\n",
    "        tb.insert(INSERT, \"\\n\")\n",
    "    except tk.TclError:\n",
    "        pass\n",
    "\n",
    "    # Create a directed graph for user interactions\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Extract user interactions from the provided data\n",
    "    for index, post in posts.iterrows():\n",
    "        submission = reddit.submission(id=post['id'])\n",
    "        submission.comments.replace_more(limit=None) \n",
    "        for comment in submission.comments.list():\n",
    "            if hasattr(comment, 'author') and hasattr(post, 'author') and comment.author:\n",
    "                parent_user = post['author'].name\n",
    "                reply_user = comment.author.name\n",
    "                G.add_edge(parent_user, reply_user)\n",
    "\n",
    "    tb.insert(INSERT, '\\n--------------------------------Network Graph-----------------------------------\\n')\n",
    "\n",
    "    # Visualize the network graph\n",
    "    pos = nx.kamada_kawai_layout(G)\n",
    "    nx.draw(G, pos, with_labels=False, node_size=100, node_color=\"orange\", edge_color=\"gray\", width=0.5)\n",
    "    plt.title('User Interaction Network for \\''+Sub+'\\'')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "top = tkinter.Tk()\n",
    "top.tk.call('encoding', 'system', 'utf-8')\n",
    "top.wm_title(\"Reddit Network Analysis\")\n",
    "\n",
    "# Create and place the elements in the grid\n",
    "L1 = Label(text=\"Subreddit Name\")\n",
    "L1.grid(row=0, column=0, sticky=\"nsew\")\n",
    "\n",
    "E1 = Entry(bd=5)\n",
    "E1.grid(row=0, column=1)\n",
    "\n",
    "B = tkinter.Button(text=\"Search\", command=reddit_graph)\n",
    "B.grid(row=0, column=2)\n",
    "\n",
    "tb = Text(top)\n",
    "tb.grid(row=1, column=0, columnspan=3)\n",
    "\n",
    "top.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247fe70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
